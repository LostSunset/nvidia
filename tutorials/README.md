
# Tutorials

 -  [Mastering GitHub Copilot for AI Paired Programming](https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming)
 -  [Multi GPU Programming Models](https://github.com/NVIDIA/multi-gpu-programming-models) 
 -  [nvCOMP docs and examples](https://github.com/NVIDIA/nvcomp)
 -  NVIDIA AI Workbench
     -  [How to customize a Stable Diffusion XL (SDXL) model](https://github.com/NVIDIA/workbench-example-sdxl-customization)
     -  [How to fine-tune a Llama v2 7B large language model](https://github.com/NVIDIA/workbench-example-llama2-finetune)
     -  [How to p-tune and prompt tune a NeMo-Megatron LLM using the NeMo Framework](https://github.com/NVIDIA/workbench-example-nemo-ptuning)
     -  [How to train a Large Language Model to annotate large sections of text with realistic punctuation and capitalization using the NeMo Framework](https://github.com/NVIDIA/workbench-example-nemo-punctuation)
     -  [Short introduction of the cuDF library](https://github.com/NVIDIA/workbench-example-rapids-cudf)
     -  [Short introduction of the cuML library](https://github.com/NVIDIA/workbench-example-rapids-cuml)
 -  [NVIDIA BioNeMo Examples](https://github.com/NVIDIA/BioNeMo)
 -  [NVIDIA Deep Learning Examples for Tensor Cores](https://github.com/NVIDIA/DeepLearningExamples)
 -  [NVIDIA Generative AI Examples](https://github.com/NVIDIA/GenerativeAIExamples)
 -  [NVIDIA Grace CPU Benchmarking Guide](https://github.com/NVIDIA/grace-cpu-benchmarking-guide)
 -  [OptiX Toolkit (OTK) Examples](https://github.com/NVIDIA/otk-examples)
 -  [RAPIDS Accelerator for Apache Spark examples](https://github.com/NVIDIA/spark-rapids-examples)

---

# [NVIDIA 免費 AI 線上課程](https://twitter.com/heyshrutimishra/status/1771206798881825005)

 -  [Generative AI Explained / 解釋生成式 AI](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-07+V1)

    課程內容：
     -  生成式 AI 的定義、工作原理
     -  各種生成式 AI 應用
     -  生成式 AI 的挑戰和機會

    課前要求：對機器學習和深度學習有基本的瞭解。

 -  [Building A Brain in 10 Minutes / 10 分鐘內構建「大腦」](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+T-FX-01+V1)

    課程內容：
     -  神經網路如何使用資料進行學習
     -  神經元背後的數學原理

    課前要求：
     -  瞭解 Python 3 中的基本程式設計概念，如函式、迴圈、字典和陣列；
     -  瞭解如何計算迴歸線。

 -  [Augment your LLM Using Retrieval Augmented Generation / 使用 RAG 增強 LLM 性能](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:NVIDIA+S-FX-16+v1)

    課程內容：
     -  RAG 基礎知識
     -  RAG 檢索過程
     -  NVIDIA AI 基礎和 RAG 模型元件

 -  [Building RAG Agents with LLMs / 使用 LLM 構建 RAG 智慧體](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-15+V1)

    課程內容：
     -  探索 LLM 和向量資料庫的可擴充套件部署策略
     -  學習微服務知識，掌握如何在微服務之間協作以及如何開發屬於自己的微服務
     -  利用 LangChain 正規化來開發對話管理和文件檢索解決方案
     -  用最先進的模型練習，明確有關產品化和框架探索的後續步驟

    適合熟悉 LLM 及其相關組合框架 (如 LangChain)，具有中級 Python 水準，最好具備網路工程和開發維運背景。

 -  [Accelerate Data Science Workflows with Zero Code Changes / 零程式碼更改加速資料科學工作流](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+T-DS-03+V1)

    課程內容：
     -  瞭解跨 CPU 和 GPU 的制式工作流在資料科學任務中的優勢
     -  學習如何在不更改程式碼的情況下，利用 GPU 加速各種資料處理和機器學習工作流程
     -  體驗當工作流程透過 GPU 加速時，處理時間顯著縮短的效果

    課前要求：
     -  對表格資料上的資料處理和標準資料科學工作流程有基本的理解
     -  有使用常見 Python 資料分析庫的經驗
     -  使用過 NVIDIA RAPIDS (cuDF, cuML, cuGraph)、pandas、scikit-learn 和 NetworkX
    
 -  [How to Perform Large-Scale Image Classification / 如何進行大規模影像分類](https://www.classcentral.com/course/youtube-grandmaster-series-how-to-perform-large-scale-image-classification-130184)

    課程內容：如何進行大規模影像分類，涵蓋挑戰、建模技術和驗證策略。

    適合資料科學家、機器學習從業者以及對深度學習感興趣的人觀看。

 -  [Mastering Recommender Systems / 掌握推薦系統](https://www.classcentral.com/course/youtube-grandmaster-series-mastering-recommender-systems-184298)

    課程內容：NVIDIA Kaggle Grandmasters 構建電子商務推薦系統的策略， 內容涵蓋 2 階段模型、候選模型生成、特徵工程和整合方法。

    適合資料科學家、機器學習工程師以及對推薦系統和資料科學感興趣的人觀看。

 -  [Introduction to AI in the Data Center / 資料中心的 AI](https://www.coursera.org/learn/introduction-ai-data-center)

    課程內容：
     -  AI 案例、機器學習、深度學習及其工作流程
     -  GPU 架構及其對 AI 的影響
     -  深度學習框架和部署注意事項

 -  [Introduction to Networking / 網路技術入門](https://www.coursera.org/learn/introduction-to-networking-nvidia)

    課程內容：
     -  瞭解網路及其重要性。
     -  探索乙太網基礎知識及乙太網網路中的資料轉發。
     -  討論網路元件、需求、OSI 模型、TCP/IP 協議。

---

# [Ilya Sutskever 的機器學習推薦清單](https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE)

> If you really learn all of these, you'll know 90% of what matters today.\
> via: <https://twitter.com/keshavchan/status/1787861946173186062>

 -  [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
 -  [The First Law of Complexodynamics](https://scottaaronson.blog/?p=762)
 -  [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
 -  [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
 -  [Recurrent Neural Network Regularization](https://arxiv.org/abs/1409.2329)
 -  [Keeping Neural Networks Simple by Minimizing the Description Length of the Weights](https://www.cs.toronto.edu/~hinton/absps/colt93.pdf)
 -  [Pointer Networks](https://arxiv.org/abs/1506.03134)
 -  [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
 -  [Order Matters: Sequence to sequence for sets](https://arxiv.org/abs/1511.06391)
 -  [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/abs/1811.06965)
 -  [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
 -  [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122)
 -  [Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212)
 -  [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
 -  [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
 -  [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)
 -  [A simple neural network module for relational reasoning](https://arxiv.org/abs/1706.01427)
 -  [Variational Lossy Autoencoder](https://arxiv.org/abs/1611.02731)
 -  [Relational recurrent neural networks](https://arxiv.org/abs/1806.01822)
 -  [Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton](https://arxiv.org/abs/1405.6903)
 -  [Neural Turing Machines](https://arxiv.org/abs/1410.5401)
 -  [Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595)
 -  [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
 -  [A tutorial introduction to the minimum description length principle](https://arxiv.org/abs/math/0406077)
 -  [Machine Super Intelligence](https://www.vetta.org/documents/Machine_Super_Intelligence.pdf)
 -  [Kolmogorov Complexity and Algorithmic Randomness](https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf)
 -  [CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/)

---

# My Collection

 -  [RAG using Llama 3 by Meta AI](https://lightning.ai/lightning-ai/studios/rag-using-llama-3-by-meta-ai?tab=overview)

    In this studio we are building a completely self-hosted "Chat with your Docs" RAG application using Llama-3, served locally through Ollama.

 -  [Transformers Tutorials](https://github.com/NielsRogge/Transformers-Tutorials)

    這份線上教學手冊是 Niels Rogge 所收納的 40 餘種各式各樣的任務模型/演算法的程式範例 (Google Colab 筆記、論文連結)

<!--
  vim:ic noet norl wrap sw=8 ts=8 sts=8 ft=markdown:
  -->
